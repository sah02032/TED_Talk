NLTK Top 10 frequent words:
['fs', 'AppleTypeServices', 'cb', 'AppleTypeServicesF', 'I', 'AI', 'And', 'think', 'objective', 'going']

NLTK Summary:
When you ask a human to get you a cup of coffee, you don\'92t mean this should be their life\'92s mission, and nothing else in the universe matters. They think, yeah, of course, if the machine does the work, then I'm going to be unemployed. And we're going to need, I think, several Einsteins to make it happen.} And it\'92s when you build machines that believe with certainty that they have the objective, that\'92s when you get this sort of psychopathic behavior. I think the problem is harder than we think. \fs28 \AppleTypeServices \cb1 \ \fs32 \AppleTypeServices\AppleTypeServicesF65539 \cb2 And the problem with the way we build AI systems now is we give them a fixed objective. \fs28 \AppleTypeServices \cb1 \ \fs32 \AppleTypeServices\AppleTypeServicesF65539 \cb2 There\'92s a big difference between asking a human to do something and giving that as the objective to an AI system.

Gensim Top 10 frequent words:
['don', 'know', 'blue', 'green', 'red', 'should', 'he', 'cssrgb', 'and', 'are']

Gensim Summary:
\fs32 \AppleTypeServices\AppleTypeServicesF65539 \cb2 There\'92s a big difference between asking a human to do something and giving that as the objective to an AI system. \fs32 \AppleTypeServices\AppleTypeServicesF65539 \cb2 And the problem with the way we build AI systems now is we give them a fixed objective. \fs32 \AppleTypeServices\AppleTypeServicesF65539 \cb2 And the reason that we don\'92t have to do that with humans is that humans often know that they don\'92t know all the things that we care about. If we build systems that know that they don\'92t know what the objective is, then they start to exhibit these behaviors, like asking permission before getting rid of all the oxygen in the atmosphere. \fs32 \AppleTypeServices\AppleTypeServicesF65539 \cb2 In all these senses, control over the AI system comes from the machine\'92s uncertainty about what the true objective is. \fs32 \AppleTypeServices\AppleTypeServicesF65539 \cb2 What happens when general purpose AI hits the real economy? \fs32 \AppleTypeServices\AppleTypeServicesF65539 \cb2 That idea, which I think it was Keynes who called it technological unemployment in 1930, is very obvious to people. They think, yeah, of course, if the machine does the work, then I'm going to be unemployed. \fs32 \AppleTypeServices\AppleTypeServicesF65539 \cb2 You can think about the warehouses that companies are currently operating for e-commerce, they are half automated. The way it works is that an old warehouse\'97 where you\'92ve got tons of stuff piled up all over the place and humans go and rummage around and then bring it back and send it off\'97 there\'92s a robot who goes and gets the shelving unit that contains the thing that you need, but the human has to pick the object out of the bin or off the shelf, because that\'92s still too difficult. \fs32 \AppleTypeServices\AppleTypeServicesF65539 \cb2 There's an interesting story that E.M. Forster wrote, where everyone is entirely machine dependent. \fs32 \AppleTypeServices\AppleTypeServicesF65539 \cb2 I think that\'92s something we have to understand as AI moves forward. \fs32 \AppleTypeServices\AppleTypeServicesF65539 \cb2 So in that sense, I think most experts say by the end of the century, we\'92re very, very likely to have general purpose AI. \fs32 \AppleTypeServices\AppleTypeServicesF65539 \cb2 I like what John McAfee, he was one of the founders of AI, when he was asked this question, he said, somewhere between five and 500 years. And we're going to need, I think, several Einsteins to make it happen.}

SpaCy Top 10 frequent words:
['\\AppleTypeServices', '\\cb1', 'AI', 'think', 't', 'going', 'objective', "don\\'92", "that\\'92s", 'humans']

SpaCy Summary:
\fs28 \AppleTypeServices \cb1 \  \fs32 \AppleTypeServices\AppleTypeServicesF65539 \cb2 There\'92s a big difference between asking a human to do something and giving that as the objective to an AI system. We don't think of this as a terribly sophisticated capability, but AI systems don\'92t have it because the way we build them now, they have to know the full objective. And the reason that we don\'92t have to do that with humans is that humans often know that they don\'92t know all the things that we care about. The way it works is that an old warehouse\'97 where you\'92ve got tons of stuff piled up all over the place and humans go and rummage around and then bring it back and send it off\'97 there\'92s a robot who goes and gets the shelving unit that contains the thing that you need, but the human has to pick the object out of the bin or off the shelf, because that\'92s still too difficult. If we build systems that know that they don\'92t know what the objective is, then they start to exhibit these behaviors, like asking permission before getting rid of all the oxygen in the atmosphere. 

