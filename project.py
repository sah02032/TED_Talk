# -*- coding: utf-8 -*-
"""Project.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/17LiPMqBeqXqrvJkvxW-El4MtbfiA85bm
"""

from gensim.summarization import summarize # version 3.8.3 has summarization
import spacy
from spacy.lang.en.stop_words import STOP_WORDS
from string import punctuation
from collections import Counter
from heapq import nlargest
import regex as re
import nltk
from gensim import corpora
from gensim.utils import simple_preprocess

nltk.download('punkt')
nltk.download('stopwords')

for i in range(1, 11):
  # read file
  f = open('/content/drive/MyDrive/CS_688/Talk/Talk{}.txt'.format(i),'r')
  wf = open('/content/drive/MyDrive/CS_688/Talk/Talk{}Summary.txt'.format(i),'w+')
  content = f.readlines()
  string = " ".join(content)

  # NLTK freq_word
  article_text = re.sub(r'\[[0-9]*\]', ' ', string)
  article_text = re.sub(r'\s+', ' ', article_text)
  format_article_text = re.sub('[^a-zA-Z]', ' ', article_text )
  format_article_text = re.sub(r'\s+', ' ', format_article_text)
  sentence_list = nltk.sent_tokenize(article_text)
  stopwords = nltk.corpus.stopwords.words('english')
  word_freq = {}
  for word in nltk.word_tokenize(format_article_text):
    if word not in stopwords:
      if word not in word_freq.keys():
        word_freq[word] = 1
      else:
        word_freq[word] += 1
  keys = list(word_freq.keys())
  keys = sorted(keys, key=word_freq.get, reverse=True)
  wf.write("NLTK Top 10 frequent words:\n" + str(keys[:10]) + "\n\n")

	# NLTK summary
  maximum_freq = max(word_freq.values())
  for word in word_freq.keys():
    word_freq[word] = (word_freq[word]/maximum_freq)
  sentence_scores = {}
  for sent in sentence_list:
    for word in nltk.word_tokenize(sent.lower()):
      if word in word_freq.keys():
        if len(sent.split(' ')) < 30:
          if sent not in sentence_scores.keys():
            sentence_scores[sent] = word_freq[word]
          else:
            sentence_scores[sent] += word_freq[word]    
  summary_sentences = nlargest(7, sentence_scores, key=sentence_scores.get)
  summary = ' '.join(summary_sentences)
  wf.write("NLTK Summary:\n" + summary + "\n\n")

	# Gensim freq_word
  doc_tokenized = [simple_preprocess(line, deacc =True) for line in content]
  dictionary = corpora.Dictionary()
  bow_corpus = [dictionary.doc2bow(doc, allow_update=True) for doc in doc_tokenized]
  id_words = [[(dictionary[id], count) for id, count in line] for line in bow_corpus]
  key_dict = []
  for a in id_words:
    key_dict.extend(a)
  key_dict = dict(key_dict)
  keys = list(key_dict.keys())
  keys = sorted(keys, key=key_dict.get, reverse=True)
  wf.write("Gensim Top 10 frequent words:\n" + str(keys[:10]) + "\n\n")

	# Gensim Summary
  summary_Gensim = summarize(string)
  wf.write("Gensim Summary:\n" + summary_Gensim.replace("\n", " ") + "\n\n")

	# SpaCy freq_words
  nlp = spacy.load("en_core_web_sm")
  doc = nlp(string)
  keyword = []
  stop = list(STOP_WORDS)
  pos_tag = ["PROPN", "ADJ", "NOUN", "VERB"]
  for token in doc:
    if (token.text not in stop and token.text not in punctuation and
		  	token.pos_ in pos_tag):
      keyword.append(token.text)
  freq_word = Counter(keyword)
  keys = list(freq_word.keys())
  keys = sorted(keys, key=freq_word.get, reverse=True)
  wf.write("SpaCy Top 10 frequent words:\n" + str(keys[:10]) + "\n\n")

	# SpaCy Summary
  max_freq = freq_word.most_common(1)[0][1]
  for word in freq_word.keys():
    freq_word[word] /= max_freq
  sent_strength = {}
  for sent in doc.sents:
    for word in sent:
      if word.text in freq_word.keys():
        if sent in sent_strength.keys():
          sent_strength[sent] += freq_word[word.text]
        else:
          sent_strength[sent] = freq_word[word.text]
  summarized_sentences = nlargest(5, sent_strength, key=sent_strength.get)
  wf.write("SpaCy Summary:\n" + " ".join([str(x).replace("\n", "") for x in summarized_sentences]) + "\n\n")

  wf.close()

